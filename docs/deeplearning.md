# Deep Learning Coursework at The University of British Columbia

## Dates
- **Start**: January 2025
- **End**: April 2025

## Links


## Summary
Evan has undertaken an extensive **deep learning curriculum** including **UBC’s CPEN 455: Deep Learning**, a course designed to provide both a **strong theoretical foundation** and **practical expertise** in **state-of-the-art (SOTA) deep learning techniques**. Building upon rigorous prerequisites in **probability, statistics, linear algebra, and optimization**, this coursework integrates **cutting-edge neural network architectures** and **advanced generative AI models**, with a **strong emphasis on PyTorch for implementation**. 

Through a combination of **theoretical lectures, mathematical derivations, hands-on programming assignments, and a research-driven course project**, Evan has developed **expertise in modern deep learning frameworks**, including **transformers, diffusion models, and deep reinforcement learning**. The coursework’s **engineering-focused applications** reinforced his ability to **apply deep learning in domains such as computer vision, sequence modeling, and autonomous decision-making**.

## Technologies Used
- **PyTorch (Primary Deep Learning Framework)**
- **Neural Network Training & Optimization**
- **Computer Vision (CNNs, UNet, ResNet, MobileNet)**
- **Sequence Modeling (RNNs, Transformers, LLMs)**
- **Generative AI (VAEs, Denoising Diffusion Models)**
- **Deep Reinforcement Learning (MDPs, Q-Learning, Policy Gradients)**

## Details
This course covered **both fundamental and advanced deep learning methodologies**, emphasizing the **mathematical underpinnings, architectural innovations, and best engineering practices** in neural network design and optimization.

### **Mathematical & Theoretical Foundations**
- **Statistical Learning & Linear Models**: The course began with **linear models for regression and classification**, providing a **strong probabilistic framework** to understand deep learning fundamentals.
- **Backpropagation & Optimization**: Topics included **gradient-based optimization (SGD, Adam, RMSProp)**, **adaptive learning rates, batch normalization, and regularization techniques (weight decay, early stopping)**.
- **Matrix Calculus & Autograd**: Deep dive into **automatic differentiation**, enabling efficient gradient computation for large-scale neural networks.

### **Modern Deep Learning Architectures**
- **Multilayer Perceptrons (MLPs)**: Fully connected networks, **dropout regularization**, and **batch normalization**.
- **Convolutional Neural Networks (CNNs)**: Implemented **CNN architectures (UNet, ResNet, MobileNet)** with **variants like transposed convolutions, dilated convolutions, and depthwise separable convolutions**.
- **Recurrent Neural Networks (RNNs) & Sequence Modeling**: Developed **sequence models**, including **LSTMs, GRUs, and attention-based architectures** for structured temporal data.
- **Transformers & Large Language Models (LLMs)**: Studied the **self-attention mechanism**, **Positional Encoding**, **BERT**, and **GPT-based architectures**.
- **Graph Neural Networks (GNNs)**: Implemented **node classification and message-passing architectures** for structured graph data.

### **Generative AI & Advanced Techniques**
- **Autoencoders & VAEs**: Trained **variational autoencoders (VAEs) and denoising autoencoders**, learning **latent space representations** for structured data generation.
- **Denoising Diffusion Models (DDPMs)**: Explored **score-based generative models**, including **denoising diffusion probabilistic models** for high-fidelity image synthesis.
- **Deep Reinforcement Learning (DRL)**: Applied **Markov Decision Processes (MDPs), Bellman Equations, Q-Learning, and Policy Gradient methods** to reinforcement learning tasks.

### **Implementation & Engineering Applications**
- **PyTorch for Neural Network Development**: 
  - Extensive use of **PyTorch’s autograd, dataset loaders, and model abstraction** for training deep learning models efficiently.
  - Implemented **custom neural network layers and training loops** for research-driven assignments.
- **Hyperparameter Optimization & Model Training**:
  - Applied **grid search, random search, and adaptive learning rate tuning** techniques to maximize model performance.
  - Used **TensorBoard and PyTorch Lightning for experiment tracking**.
- **Hugging Face Transformers & LLM Integration**:
  - Fine-tuned **pretrained transformers (BERT, GPT-2, ViT)** for **language modeling and image captioning**.
  - Implemented **parameter-efficient transfer learning techniques**.


### **Project Impact & Future Recommendations**
Through **rigorous coursework, mathematical depth, and hands-on PyTorch development**, Evan has built a **strong theoretical and practical understanding of deep learning**, positioning him to **leverage AI for real-world engineering applications**.

By mastering **theory, implementation, and real-world applications**, Evan has developed **expertise in deep learning**, preparing him for **engineering-driven AI research and industrial-scale deployment**.
