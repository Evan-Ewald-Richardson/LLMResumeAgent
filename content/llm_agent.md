---
type: project
title: "Personal LLM Chatbot with RAG for Experience-Based Q&A"
date_start: "2025-02"
skills: [LLM Deployment, RAG (Retrieval-Augmented Generation), API Development, FastAPI, Prompt Engineering, Cloud & Local Server Hosting]
technologies: [Mistral 7B, DeepSeek-R1:8B, FastAPI, Ollama, Retrieval-Augmented Generation (RAG), Cloudflare, Webflow, Vector Databases]
keywords: [LLM, Chatbot, AI-Powered Resume, Knowledge Retrieval, Self-Hosted AI]
---

# Personal LLM Chatbot with RAG for Experience-Based Q&A

## Overview
Developing a **self-hosted LLM-powered chatbot** designed to answer questions about **career experience, projects, and education**. The chatbot integrates **Retrieval-Augmented Generation (RAG)** to enhance **response relevance** by indexing and retrieving structured experience data.

- Running **Mistral 7B / DeepSeek-R1:8B** locally via **Ollama**, optimized for **fast, efficient responses**.  
- Implemented **RAG pipeline** to dynamically search and retrieve **structured project and work experience** before generating responses.  
- Integrated chatbot with **Webflow-hosted resume** via a **FastAPI backend and Cloudflare for routing**.  

## Technical Details
Built an **LLM-powered assistant** optimized for **knowledge retrieval, response filtering, and efficient local hosting**.

- **LLM Hosting & API Deployment**:  
  - Deployed **Mistral 7B / DeepSeek-R1:8B** on a **local server** for **cost-efficient inference**.  
  - Designed a **FastAPI-based API** to expose the chatbot for **resume Q&A interactions**.  
  - Configured **Cloudflare DNS and proxy settings** for secure API routing.  

- **Retrieval-Augmented Generation (RAG) Pipeline**:  
  - Embedded **structured resume data** in a **vector database** for real-time retrieval.  
  - Used **semantic search and ranking** to retrieve **relevant experience details** before generating responses.  
  - Applied **prompt engineering techniques** to optimize **accuracy and coherence**.  

- **Server Optimization & Performance Tuning**:  
  - Experimented with **GPU vs CPU inference** to optimize latency.  
  - Fine-tuned **token limits, response formatting, and search accuracy** to enhance usability.  
  - Integrated **query pre-processing** to filter out **irrelevant questions**.  

## Skills Demonstrated
Applied **LLM development, API engineering, and retrieval-augmented generation** to build an **AI-powered resume assistant**.

- **LLM Optimization & Hosting**: Deployed and tuned **Mistral 7B / DeepSeek-R1:8B** for local inference.  
- **API Development & Cloud Integration**: Built a **FastAPI-based LLM interface**, integrated with **Cloudflare**.  
- **Retrieval-Augmented Generation (RAG)**: Indexed **structured data** for **context-aware responses**.  

## Quantitative Outcomes
- **Implemented a real-time LLM chatbot** for **career experience Q&A**.  
- **Reduced API response latency** by optimizing **local inference and retrieval pipeline**.  
- **Deployed and integrated LLM services** with **Webflow and Cloudflare-based routing**.  

## Additional Context
- **Project Motivation**: Create an **interactive AI-powered resume assistant** for career and technical Q&A.  
- **Future Enhancements**:  
  - **Fine-tune response formatting** for improved professional tone.  
  - **Expand RAG capabilities** to handle **broader knowledge queries**.  
  - **Optimize GPU inference performance** for improved API speed.  
